# Interview Preparation Guide: Part 3
## Advanced Autoscaling & Production CI/CD Pipeline
**Sections 17 & 21: Karpenter, GitHub Actions, ArgoCD**

**Date**: February 9, 2026  
**Status**: Complete Interview Preparation Guide

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [Section 17: Karpenter - Advanced Node Autoscaling](#section-17-karpenter---advanced-node-autoscaling)
3. [Section 21: DevOps CI/CD with GitHub Actions & ArgoCD](#section-21-devops-cicd-pipeline)
4. [Complete Automation Workflow](#complete-automation-workflow)
5. [Interview Q&A - Part 3](#interview-qa---part-3)

---

## Executive Summary

You implemented **enterprise-grade automation** for:

âœ… **Cluster Autoscaling** - Karpenter (seconds vs minutes, cost optimization with Spot)  
âœ… **Continuous Integration** - GitHub Actions (build, test, push to ECR)  
âœ… **Continuous Deployment** - ArgoCD (GitOps, automatic sync, Helm integration)  
âœ… **OIDC Authentication** - Secure AWS access without hardcoded keys  
âœ… **Zero-Downtime Deployments** - Rolling updates with automatic rollback  

---

## Section 17: Karpenter - Advanced Node Autoscaling

### Problem: Traditional Node Autoscaling is Slow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CLUSTER AUTOSCALER (Traditional Kubernetes)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚ Time 0:00  Pod fails to schedule (no capacity)                â”‚
â”‚            â†“                                                   â”‚
â”‚ Time 0:30  Autoscaler detects unscheduled pods                â”‚
â”‚            â†“                                                   â”‚
â”‚ Time 0:45  Request EC2 instances from Auto Scaling Group (ASG)â”‚
â”‚            â†“                                                   â”‚
â”‚ Time 2:00  EC2 instances launched & booted                    â”‚
â”‚            â†“                                                   â”‚
â”‚ Time 2:30  Nodes registered with Kubernetes                   â”‚
â”‚            â†“                                                   â”‚
â”‚ Time 3:00  Pods scheduled and running                         â”‚
â”‚            â†“                                                   â”‚
â”‚ Result:    3 MINUTES from request to running pods âŒ SLOW     â”‚
â”‚            User timeout, customer impact                       â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ KARPENTER (Modern Node Provisioning)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚ Time 0:00  Pod fails to schedule (no capacity)                â”‚
â”‚            â†“                                                   â”‚
â”‚ Time 0:02  Karpenter detects unschedulable pods               â”‚
â”‚            â†“                                                   â”‚
â”‚ Time 0:04  Makes API call to EC2 (parallel provisioning)      â”‚
â”‚            â†“                                                   â”‚
â”‚ Time 0:15  EC2 instances launched & booted                    â”‚
â”‚            â†“                                                   â”‚
â”‚ Time 0:20  Nodes registered with Kubernetes                   â”‚
â”‚            â†“                                                   â”‚
â”‚ Time 0:25  Pods scheduled and running                         â”‚
â”‚            â†“                                                   â”‚
â”‚ Result:    25 SECONDS from request to running pods âœ… FAST!  â”‚
â”‚            Near zero interruption to user                      â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Karpenter Architecture

```
KARPENTER PROVISIONING FLOW
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. MONITORING PHASE
   â”œâ”€ Karpenter controller runs in kube-system namespace
   â”œâ”€ Watches for unschedulable pods in cluster
   â”œâ”€ Also watches for underutilized nodes
   â””â”€ Continuously evaluates cluster utilization

2. DECISION MAKING
   When pod fails to schedule:
   â”œâ”€ Analyzes pod requirements:
   â”‚  â”œâ”€ CPU: 500m
   â”‚  â”œâ”€ Memory: 512Mi
   â”‚  â”œâ”€ Instance type hints: t3, t4, m5 (optional)
   â”‚  â””â”€ Zone preference: any AZ
   â”‚
   â”œâ”€ Checks NodePool configuration:
   â”‚  â”œâ”€ On-Demand pool: cheap, reliable (primary)
   â”‚  â”œâ”€ Spot pool: cheaper, risk of interruption (secondary)
   â”‚  â””â”€ Instance type recommendations: consolidate or split
   â”‚
   â””â”€ Selects best instance offering:
      â””â”€ On-Demand: t3.medium (smallest that fits)
         Cost: $0.0416/hour
         Launch time: ~15 seconds

3. PROVISIONING PHASE
   â”œâ”€ Create EC2 instances (using EC2 FleetRequest API)
   â”œâ”€ Wait for boot (cloud-init runs, kubelet starts)
   â”œâ”€ Register with Kubernetes (CSR signed automatically)
   â””â”€ Kubelet joins cluster

4. SCHEDULING PHASE
   â”œâ”€ New node joins cluster
   â”œâ”€ kube-scheduler places waiting pods on new node
   â”œâ”€ Pods transition: Pending â†’ Running
   â””â”€ Application starts

5. CONSOLIDATION PHASE
   Periodically (every 30 seconds):
   â”œâ”€ Identify underutilized nodes
   â”œâ”€ Drain pods gracefully
   â”œâ”€ Delete empty nodes
   â”œâ”€ Cost optimization âœ…
   â””â”€ Removes Spot instances before interruption

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Detailed Step-by-Step: Karpenter Node Provisioning (Pod to Running)**

```
STEP 1: POD CREATED, SCHEDULER EVALUATES
â””â”€ Developer applies: kubectl apply -f deployment.yaml
â””â”€ Deployment controller creates pods
â””â”€ Scheduler receives: Pod.catalog-5dcb7bb4f-xyz pending
   â”œâ”€ Pod requirements: CPU 800m, Memory 1Gi, zone: any AZ
   â”œâ”€ Pod requests: should run on a node âœ…
   â””â”€ Current cluster nodes:
      â”œâ”€ Node 1: 2CPU, 2Gi memory â†’ FULL (other pods hogging resources)
      â”œâ”€ Node 2: 1CPU, 0.5Gi memory â†’ FULL (no room)
      â””â”€ Result: No node fits this pod âŒ UNSCHEDULABLE

STEP 2: KARPENTER DETECTS UNSCHEDULABLE POD
â””â”€ Karpenter controller watching: karpenter.sh/capacity-type = on-demand
â””â”€ Event: Pod.catalog pending > 1 second in Pending state
â””â”€ Query: Why is it pending?
   â””â”€ API call: kubectl describe pod catalog-5dcb7bb4f-xyz
   â””â”€ Response: "0/2 nodes are available: 2 Insufficient cpu"
â””â”€ Decision trigger: YES, we need more capacity
â””â”€ Consolidation check: Can we remove any underutilized nodes first?
   â”œâ”€ Node 1: 40% utilized â†’ No (still busy)
   â”œâ”€ Node 2: 25% utilized â†’ No (has capacity for small pods)
   â””â”€ Result: Cannot consolidate, must provision new
â””â”€ Karpenter status: **PROVISIONING MODE**

STEP 3: SELECT INSTANCE TYPE & OFFERING
â””â”€ Karpenter evaluates: what machines can fit this pod?
   â”œâ”€ Pod needs: CPU 800m, Memory 1Gi
   â”œâ”€ NodePool configuration:
   â”‚  â”œâ”€ Family: [t3, t4, m5, m6, c5, c6]
   â”‚  â”œâ”€ Capacity type: on-demand (primary)
   â”‚  â”œâ”€ Zone: us-east-1a, us-east-1b, us-east-1c
   â”‚  â””â”€ Instance sizes: t3.small to m5.2xlarge
   â”‚
   â””â”€ Instance fit analysis:
      â”œâ”€ t3.micro: 1 CPU, 1Gi RAM â†’ fits, cheapest $0.0104/hr
      â”œâ”€ t3.small: 2 CPU, 2Gi RAM â†’ fits, $0.0208/hr
      â”œâ”€ m5.large: 2 CPU, 8Gi RAM â†’ fits, $0.096/hr
      â”‚
      â””â”€ Best choice: t3.small
         â”œâ”€ Reason: fits pod requests (800m < 2000m CPU available)
         â”œâ”€ Consolidation-friendly: other pods can fit on it
         â”œâ”€ Cost: $0.0208/hour vs $0.0416/hour for larger
         â””â”€ AWS region: us-east-1b (default zone in config)

STEP 4: REQUEST EC2 INSTANCES (AWS API CALL)
â””â”€ Karpenter initiates: EC2 Fleet Request API call
â””â”€ Request parameters:
   â”œâ”€ Action: CreateFleet
   â”œâ”€ InstanceType: t3.small
   â”œâ”€ SubnetId: subnet-1a2b3c4d (us-east-1b)
   â”œâ”€ ImageId: ami-0a3c5c (Amazon EKS Optimized AMI)
   â”œâ”€ IamInstanceProfile: karpenter-node-role
   â”œâ”€ SecurityGroupIds: [sg-1a2b3c4d (EKS node security group)]
   â”œâ”€ TagSpecifications:
   â”‚  â”œâ”€ karpenter.sh/provisioner: default
   â”‚  â”œâ”€ karpenter.sh/capacity-type: on-demand
   â”‚  â”œâ”€ kubernetes.io/cluster/kalyan-cluster: owned
   â”‚  â””â”€ Name: karpenter-node-1a2b3c4d
   â”‚
   â””â”€ Result: Fleet starts launching instance
      â”œâ”€ AWS state: pending (instance being allocated)
      â””â”€ Karpenter state: WAITING_FOR_INSTANCE

STEP 5: EC2 INSTANCE BOOTS (AWS INFRASTRUCTURE) â±ï¸ ~10-15 seconds
â””â”€ EC2 hypervisor allocates vCPU & memory
â””â”€ Instance starts booting
â””â”€ Boot sequence:
   â”œâ”€ t3.small: 2 vCPU, 2Gi RAM allocated
   â”œâ”€ Network interface attached (eth0: private IP 10.0.2.100/24)
   â”œâ”€ Storage: 20Gi EBS gp3 volume (fast, optimized)
   â”œâ”€ Security group applied (allows kubelet port 10250, SSH 22)
   â””â”€ Instance begins loading OS kernel
â””â”€ AWS EC2 state: running âœ…

STEP 6: CLOUD-INIT & KUBELET STARTUP â±ï¸ ~5-10 seconds
â””â”€ Linux kernel boots
â””â”€ Cloud-init executes (user-data script from EKS AMI):
   â”œâ”€ Export environment variables:
   â”‚  â”œâ”€ AWS_DEFAULT_REGION=us-east-1
   â”‚  â”œâ”€ CLUSTER_NAME=kalyan-cluster
   â”‚  â”œâ”€ KUBELET_CONFIG=/etc/kubernetes/kubelet/kubelet-config.json
   â”‚  â””â”€ NODE_ROLE=default (matches karpenter provisioner)
   â”‚
   â”œâ”€ Bootstrap kubelet:
   â”‚  â”œâ”€ Download kubeconfig from AWS API Server
   â”‚  â”‚  â””â”€ endpoint: https://kalyan-cluster.eks.us-east-1.amazonaws.com
   â”‚  â”œâ”€ Start kubelet service: systemctl start kubelet
   â”‚  â”œâ”€ Register node with cluster: CSR (Certificate Signing Request) sent
   â”‚  â””â”€ CSR auto-signed by AWS (EC2 controller approves)
   â”‚
   â””â”€ Result: Node successfully joined cluster
      â”œâ”€ Node status: Ready (after health checks)
      â””â”€ Kubelet state: running, listening on :10250

STEP 7: KARPENTER MONITORS EC2 â†’ KUBERNETES TRANSITION
â””â”€ Karpenter polls: kubectl get nodes
â””â”€ Query EC2 API: DescribeInstances (instance-id from fleet)
â””â”€ Correlation: EC2 instance i-1a2b3c4d â†’ Kubernetes Node karpenter-node-1a2b3c4d
â””â”€ Node status check:
   â”œâ”€ NodeReady condition: False â†’ True (takes 10-30 seconds)
   â”œâ”€ Node resources: allocatable CPU 1950m, Memory 1876Mi
   â”œâ”€ Kubelet status: Ready âœ…
   â””â”€ Taints: karpenter.sh/capacity-type=on-demand:NoSchedule
      â””â”€ This prevents OTHER schedulers from using this node
      â””â”€ Only Karpenter scheduler removes taint after verification

STEP 8: KARPENTER PROVISIONING OBJECT REGISTERED
â””â”€ Create Kubernetes resource: karpenter.sh/NodePool object
â””â”€ Status: NodePool.status.nodes += 1
â””â”€ Record: Karpenter.status.summary:
   â”œâ”€ capacity-type: on-demand
   â”œâ”€ providerName: default  
   â”œâ”€ instance-type: t3.small
   â”œâ”€ zone: us-east-1b
   â”œâ”€ available-capacity: CPU 1950m, Memory 1876Mi
   â””â”€ nodes-created: 1
â””â”€ Karpenter state: NODE_PROVISIONED âœ…

STEP 9: KUBERNETES SCHEDULER PLACES POD ON NEW NODE
â””â”€ Scheduler runs its evaluation again: kubectl get nodes
â””â”€ New node available: karpenter-node-1a2b3c4d (Ready)
   â”œâ”€ Available CPU: 1950m (pod needs 800m) âœ“
   â”œâ”€ Available Memory: 1876Mi (pod needs 1Gi) âœ“
   â””â”€ Taints: none (Karpenter removed NoSchedule taint)
â”‚
â””â”€ Scheduler decision: PLACE Pod.catalog on new node
   â”œâ”€ Pod.spec.nodeName = karpenter-node-1a2b3c4d
   â”œâ”€ Pod status: Pending â†’ Running
   â””â”€ kubelet on new node pulls image & starts container
      â”œâ”€ Docker image pull: catalog:sha-abc123 (5-10 sec)
      â”œâ”€ Container init: database connections established
      â”œâ”€ Readiness probe: GET /api/ready â†’ 200 OK âœ…
      â””â”€ Pod fully running âœ…

STEP 10: APPLICATION SERVING TRAFFIC
â””â”€ Pod now running on karpenter-node-1a2b3c4d:
   â”œâ”€ Pod IP: 10.0.2.101
   â”œâ”€ Pod is ready for traffic
   â”œâ”€ Service endpoint updated: 10.0.2.101:8080
   â””â”€ Users can access application âœ…
â”‚
â””â”€ Metrics updated:
   â”œâ”€ Cluster CPU utilization: was 85% â†’ now 45% (more capacity)
   â”œâ”€ Cluster Memory utilization: was 88% â†’ now 62%
   â”œâ”€ New node memory reserved: 4% for system components
   â”œâ”€ Pod count increased: 12 â†’ 13 pods
   â””â”€ Node count increased: 2 â†’ 3 nodes

STEP 11: ONGOING KARPENTER CONSOLIDATION (30-60 seconds later)
â””â”€ Karpenter consolidation loop checks:
   â”œâ”€ Node 1: 45% utilized
   â”œâ”€ Node 2: 30% utilized
   â”œâ”€ Node 3 (new): 35% utilized
   â””â”€ Analysis: Can we consolidate (remove underutilized nodes)?
â”‚
â””â”€ Consolidation simulation:
   â”œâ”€ If we remove Node 2, can its pods fit on other nodes?
   â”‚  â”œâ”€ Node 2 pods: 5 pods requiring 300m CPU, 0.5Gi RAM each
   â”‚  â”œâ”€ Available on Node 1: 1000m CPU, 1Gi RAM â†’ YES, fits âœ“
   â”‚  â”œâ”€ Action: cordon Node 2 (no new pods scheduled)
   â”‚  â”œâ”€ Drain Node 2: evictPod â†’ pods rescheduled to Node 1
   â”‚  â”œâ”€ Terminate EC2 instance: i-2a3b4c5d
   â”‚  â””â”€ Result: saved $0.096/hour (assuming m5.large)
   â”‚
   â””â”€ Cost optimization: Karpenter automatically saves money âœ…
      â”œâ”€ Provisioning: responsive to demand
      â”œâ”€ Consolidation: aggressive cost optimization
      â”œâ”€ Spot instances: 70% cheaper than On-Demand (if enabled)
      â”œâ”€ Zero manual intervention: fully automated
      â””â”€ Result: same reliability, lower costs

TOTAL TIME: Pod Pending â†’ Pod Running: **25-35 seconds** âš¡
TRADITIONAL CLUSTER AUTOSCALER: **3-5 minutes** âŒ
TIME SAVED PER SCALE-UP EVENT: **2.5-4.5 minutes** ðŸŽ¯

KARPENTER ADVANTAGES:
âœ… Fast: seconds instead of minutes
âœ… Efficient: rightsizes instances to workload
âœ… Cost-optimized: consolidation + Spot instances
âœ… Seamless: no manual intervention needed
âœ… Responsive: handles traffic spikes instantly
```

### Karpenter Setup: Terraform

**Karpenter Infrastructure Setup Workflow:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TERRAFORM KARPENTER SETUP FILES                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

c6_01_karpenter_controller_iam_role.tf
â”œâ”€ IAM role for Karpenter controller pod
â”œâ”€ Trust relationship: pods.eks.amazonaws.com
â”œâ”€ Permissions: EC2 provisioning, spot management
â””â”€ Output: role ARN â†’ used in c6_06

c6_02_karpenter_node_iam_role.tf
â”œâ”€ IAM role for EC2 nodes created by Karpenter
â”œâ”€ Attached policies:
â”‚  â”œâ”€ AmazonEKSWorkerNodePolicy
â”‚  â”œâ”€ AmazonEKS_CNI_Policy
â”‚  â””â”€ AmazonEC2ContainerRegistryReadOnly
â””â”€ Output: role name â†’ referenced in EC2NodeClass

c6_03_karpenter_sqs_queue.tf
â”œâ”€ SQS queue for interruption notices
â”œâ”€ Receives: Spot interruption warnings
â”œâ”€ Retention: 5 minutes
â””â”€ Output: queue name â†’ Karpenter monitors

c6_04_karpenter_eventbridge_rules.tf
â”œâ”€ EventBridge rule 1: Spot interruption notices
â”‚  â””â”€ Routes to: SQS queue
â”œâ”€ EventBridge rule 2: Instance state changes
â”‚  â””â”€ Routes to: SQS queue
â””â”€ Result: Karpenter gets AWS eventsâ†’graceful draining

c6_05_karpenter_helm_chart.tf  
â”œâ”€ Helm chart: Deploys karpenter controller pod
â”œâ”€ Namespace: karpenter
â”œâ”€ Config:
â”‚  â”œâ”€ Service account with IAM role
â”‚  â”œâ”€ SQS queue name
â”‚  â””â”€ Cluster name
â””â”€ Result: Karpenter running & listening to events

         â†“

Config files cascade: Role â†’ Node Role â†’ Queue â†’ Events â†’ Helm
All connected: IAM roles â†’ EC2 provisioning â†’ Node creation
```

```hcl
# 03_KARPENTER_terraform-manifests/c6_01_karpenter_controller_iam_role.tf

resource "aws_iam_role" "karpenter_controller" {
  name = "karpenter-controller-role"
  
  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Effect = "Allow"
      Principal = {
        Service = "pods.eks.amazonaws.com"  # Pod Identity
      }
      Action = "sts:AssumeRole"
    }]
  })
}

# Custom policy for EC2 provisioning
resource "aws_iam_role_policy" "karpenter_controller_policy" {
  name = "karpenter-controller-policy"
  role = aws_iam_role.karpenter_controller.id
  
  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "ec2:CreateFleet",           # Create instance groups
          "ec2:CreateLaunchTemplate",  # Template for instances
          "ec2:CreateSpotDatafeedSubscription",
          "ec2:DescribeFleetHistory",
          "ec2:DescribeFleets",
          "ec2:DescribeImages",        # Get AMI info
          "ec2:DescribeInstanceTypes", # Get instance type specs
          "ec2:DescribeInstances",
          "ec2:DescribeKeyPairs",
          "ec2:DescribeLaunchTemplates",
          "ec2:DescribeSecurityGroups",
          "ec2:DescribeSpotPriceHistory",
          "ec2:DescribeSubnets",
          "ec2:DescribeTags",
          "ec2:DescribeVolumes",
          "ec2:GetSpotDatafeedHistory",
          "ec2:RunInstances",          # Launch instances
          "ec2:RunScheduledInstances",
          "ec2:TerminateInstances"     # Delete instances
        ]
        Resource = "*"
      },
      {
        Effect = "Allow"
        Action = [
          "iam:PassRole"               # Pass node IAM role
        ]
        Resource = "arn:aws:iam::${data.aws_caller_identity.current.account_id}:role/karpenter-node-role"
      },
      {
        Effect = "Allow"
        Action = [
          "ssm:GetParameter",          # Get AMI from SSM
          "ssm:GetParameters"
        ]
        Resource = "arn:aws:ssm:${var.aws_region}::parameter/aws/service/eks/optimized-ami*"
      }
    ]
  })
}

---

# c6_04_karpenter_node_iam_role.tf

resource "aws_iam_role" "karpenter_node" {
  name = "karpenter-node-role"
  
  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Effect = "Allow"
      Principal = {
        Service = "ec2.amazonaws.com"
      }
      Action = "sts:AssumeRole"
    }]
  })
}

# Node needs basic Kubernetes permissions
resource "aws_iam_role_policy_attachment" "karpenter_node_policy" {
  role       = aws_iam_role.karpenter_node.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"
}

resource "aws_iam_role_policy_attachment" "karpenter_node_cni_policy" {
  role       = aws_iam_role.karpenter_node.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"
}

resource "aws_iam_role_policy_attachment" "karpenter_node_registry_policy" {
  role       = aws_iam_role.karpenter_node.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
}

---

# c6_06_karpenter_helm_install.tf

resource "helm_release" "karpenter" {
  name            = "karpenter"
  repository      = "oci://public.ecr.aws/karpenter"
  chart           = "karpenter"
  namespace       = "karpenter"
  create_namespace = true
  version         = "v0.31.0"
  
  # Controller service account with IAM role
  set {
    name  = "serviceAccount.annotations.eks\\.amazonaws\\.com/role-arn"
    value = aws_iam_role.karpenter_controller.arn
  }
  
  set {
    name  = "settings.clusterName"
    value = var.cluster_name
  }
  
  set {
    name  = "settings.interruptionQueue"
    value = aws_sqs_queue.karpenter_interruption.name
  }
}

---

# c6_07_karpenter_sqs_queue.tf & c6_08_karpenter_eventbridge_rules.tf

# SQS Queue for Spot Interruption Notices
resource "aws_sqs_queue" "karpenter_interruption" {
  name                      = "karpenter-interruption-queue"
  message_retention_seconds = 300
  sqs_managed_sse_enabled   = true
}

# EventBridge Rule: Spot Instance Interruption Notice
resource "aws_cloudwatch_event_rule" "karpenter_spot_interruption" {
  name        = "karpenter-spot-interruption"
  description = "Karpenter spot interruption warning"
  
  event_pattern = jsonencode({
    source      = ["aws.ec2"]
    detail-type = ["EC2 Spot Instance Interruption Warning"]
  })
}

resource "aws_cloudwatch_event_target" "karpenter_spot_to_sqs" {
  rule      = aws_cloudwatch_event_rule.karpenter_spot_interruption.name
  target_id = "KarpenterSpotInterruptionQueue"
  arn       = aws_sqs_queue.karpenter_interruption.arn
}

# EventBridge Rule: Instance State Changes
resource "aws_cloudwatch_event_rule" "karpenter_instance_state" {
  name        = "karpenter-instance-state"
  description = "Karpenter instance state changes"
  
  event_pattern = jsonencode({
    source      = ["aws.ec2"]
    detail-type = ["EC2 Instance State-change Notification"]
  })
}

resource "aws_cloudwatch_event_target" "karpenter_state_to_sqs" {
  rule      = aws_cloudwatch_event_rule.karpenter_instance_state.name
  target_id = "KarpenterStateChangeQueue"
  arn       = aws_sqs_queue.karpenter_interruption.arn
}
```

### Karpenter Configuration: NodePool & EC2NodeClass

**Karpenter Decision Tree: NodePool â†’ EC2NodeClass â†’ Instance Selection:**

```
Pod Fails to Schedule (insufficient capacity)
                 â”‚
                 â†“
   Karpenter Controller Analyzes:
   â”œâ”€ Pod CPU: 500m
   â”œâ”€ Pod Memory: 512Mi
   â””â”€ Pod requirements (affinity, node selectors)
                 â”‚
                 â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Check Which NodePool to Use:               â”‚
   â”‚                                            â”‚
   â”‚ NodePool: on-demand (weight 50)            â”‚
   â”‚  â””â”€ Preferred for production               â”‚
   â”‚                                            â”‚
   â”‚ NodePool: spot (weight 10)                 â”‚
   â”‚  â””â”€ Lower weight (fallback option)         â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚ (pod doesn't specify nodePool)
                    â””â”€ Use on-demand (higher weight)
                    â”‚
                    â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ EC2NodeClass: default                      â”‚
   â”‚  â”œâ”€ AMI: Amazon Linux 2 (EKS optimized)   â”‚
   â”‚  â”œâ”€ Role: karpenter-node-role             â”‚
   â”‚  â”œâ”€ Subnets: auto-discover                â”‚
   â”‚  â”œâ”€ Security Groups: auto-discover        â”‚
   â”‚  â”œâ”€ EBS: 100GB gp3 encrypted             â”‚
   â”‚  â””â”€ Monitoring: detailed CloudWatch       â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ NodePool Requirements: Evaluate            â”‚
   â”‚                                            â”‚
   â”‚ â”œâ”€ Architecture: amd64 âœ“                  â”‚
   â”‚ â”œâ”€ Instance types: t3.* or t4g.* âœ“        â”‚
   â”‚ â”œâ”€ Capacity type: on-demand âœ“             â”‚
   â”‚ â”œâ”€ EBS optimized: true âœ“                 â”‚
   â”‚ â””â”€ CPU limit: 1000m (have 0m) âœ“          â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â†“
    Karpenter selects best instance:
    â”œâ”€ Cost: cheapest first
    â”œâ”€ Fit: smallest that fits
    â”‚  â””â”€ 500m CPU â†’ t3.medium (2vCPU, $0.0416/h)
    â”œâ”€ Region: same AZs as existing nodes
    â””â”€ Launch via EC2 API â†’ 15-30 seconds âš¡
```

```yaml
---
# EC2NodeClass: Template for EC2 instances
apiVersion: ec2.karpenter.sh/v1beta1
kind: EC2NodeClass
metadata:
  name: default
spec:
  # AMI selection
  amiFamily: AL2          # Amazon Linux 2 (EKS Optimized)
  role: "karpenter-node-role"
  
  # Networking
  subnetSelector:
    karpenter.sh/discovery: "true"  # Uses subnet tagged with this
  
  securityGroupSelector:
    karpenter.sh/discovery: "true"  # Uses SG tagged with this
  
  # Tagging
  tags:
    ManagedBy: karpenter
    Environment: production
  
  # User data (scripts to run on instance boot)
  userData: |
    #!/bin/bash
    echo "Karpenter provisioned node"
  
  # EBS volume configuration
  blockDeviceMappings:
  - deviceName: /dev/xvda
    ebs:
      volumeSize: 100Gi
      volumeType: gp3
      iops: 3000
      throughput: 125
      encrypted: true
      deleteOnTermination: true
  
  # MetadataOptions (IMDS)
  metadataOptions:
    httpEndpoint: enabled
    httpProtocolIPv6: disabled
    httpPutResponseHopLimit: 2
  
  # Monitoring
  detailedMonitoring: true

---
# NodePool: On-Demand instances (primary)
apiVersion: karpenter.sh/v1beta1
kind: NodePool
metadata:
  name: on-demand
spec:
  template:
    spec:
      # Link to EC2NodeClass
      nodeClassRef:
        name: default
      
      requirements:
      # Instance families (prefer newer generations)
      - key: kubernetes.io/arch
        operator: In
        values: ["amd64"]
      
      - key: node.kubernetes.io/instance-type
        operator: In
        values: ["t3.medium", "t3.large", "t4g.medium", "t4g.large"]
      
      - key: karpenter.sh/capacity-type
        operator: In
        values: ["on-demand"]  # Only on-demand, not Spot
      
      - key: karpenter.sh/ebs-optimized
        operator: In
        values: ["true"]       # Use EBS-optimized instances
      
      - key: kubernetes.io/arch
        operator: In
        values: ["amd64"]
  
  # Limits
  limits:
    resources:
      cpu: 1000m            # Total CPU allowed for this pool
      memory: 1000Gi        # Total memory allowed
  
  # Pricing behavior
  providerRef:
    name: default
  
  # Consolidation (scale down)
  consolidationPolicy:
    nodes: "false"          # Disable consolidation for on-demand
  
  # TTL: prevent permanently running nodes
  ttlSecondsAfterEmpty: 30  # 30 seconds without pods = delete node
  ttlSecondsUntilExpired: 604800  # 7 days max node age

---
# NodePool: Spot instances (secondary, optimized for cost)
apiVersion: karpenter.sh/v1beta1
kind: NodePool
metadata:
  name: spot
spec:
  weight: 50  # Lower weight (on-demand preferred)
  
  template:
    spec:
      nodeClassRef:
        name: default
      
      requirements:
      - key: kubernetes.io/arch
        operator: In
        values: ["amd64"]
      
      - key: node.kubernetes.io/instance-type
        operator: In
        values: ["t3.medium", "t3.large", "t4g.medium", "t4g.large"]
      
      - key: karpenter.sh/capacity-type
        operator: In
        values: ["spot"]      # Only Spot instances
      
      - key: karpenter.sh/ebs-optimized
        operator: In
        values: ["true"]
  
  limits:
    resources:
      cpu: 2000m            # More CPU allowed (cheaper)
      memory: 2000Gi
  
  # Disruption budget for Spot
  disruption:
    consolidateAfter: 30s
    expireAfter: 604800s    # 7 days
    budgets:
    - duration: 5m
      nodes: 10%            # Max 10% of nodes disrupted per 5 min
      reasons:
      - "Underutilized"
      - "Empty"
  
  ttlSecondsAfterEmpty: 30
  ttlSecondsUntilExpired: 604800

---
# Pod Scheduling: Prefer cheaper Spot instances for flexible workloads
apiVersion: v1
kind: Pod
metadata:
  name: catalog-spot
spec:
  # Require Spot nodes
  nodeSelector:
    karpenter.sh/capacity-type: "spot"
  
  # Or use affinity for softer constraint
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
          - key: karpenter.sh/capacity-type
            operator: In
            values: ["spot"]
```

### Karpenter Benefits Summary

```
COMPARISON: Cluster Autoscaler vs Karpenter
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Feature                  â”‚ Cluster Autoscaler â”‚ Karpenter
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Scaling Time            â”‚ 2-3 minutes        â”‚ 15-30 seconds âœ…
Bin Packing             â”‚ Limited            â”‚ Excellent âœ…
Spot Instance Support   â”‚ Complex            â”‚ Built-in âœ…
Interruption Handling   â”‚ Manual             â”‚ Automatic âœ…
Instance Type Selection â”‚ Manual ASG config  â”‚ Auto-optimized âœ…
Consolidation           â”‚ Slow               â”‚ Fast âœ…
Configuration           â”‚ AWS ASG (complex)  â”‚ K8s CRDs (simple) âœ…
Downtime                â”‚ Yes                â”‚ Zero-downtime âœ…
Cost Savings            â”‚ Moderate           â”‚ High (Spot) âœ…

COST SAVINGS EXAMPLE (100 pods, 2 vCPU each)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Cluster Autoscaler:
â”œâ”€ 20 on-demand t3.xlarge (4 vCPU each)
â”œâ”€ Cost: $0.1664/hour Ã— 20 = $3.328/hour
â”œâ”€ Monthly: $3.328 Ã— 730 = $2,429.44

Karpenter (optimized):
â”œâ”€ 15 on-demand t3.large (2 vCPU each) = 30 vCPU
â”œâ”€ 5 spot t3.xlarge (4 vCPU each) = 20 vCPU
â”œâ”€ Spot is 70% cheaper
â”œâ”€ Cost: ($0.0832 Ã— 15 + $0.0249 Ã— 5) Ã— 730
â”œâ”€ Cost: ($1.248 + $0.1245) Ã— 730
â”œâ”€ Cost: $1.3725 Ã— 730 = $1,001.93/month
â”œâ”€ **SAVINGS: $1,427.51/month (59% reduction!) âœ…**

With hundreds of pods, savings multiply significantly
```

---

## Section 21: DevOps CI/CD Pipeline

### Problem: Manual Deployment is Error-Prone

```
âŒ MANUAL DEPLOYMENT (Before CI/CD):
1. Developer writes code, commits to GitHub
2. Team member manually:
   â”œâ”€ Pulls code from GitHub
   â”œâ”€ Runs tests locally
   â”œâ”€ Builds Docker image manually
   â”œâ”€ Tags image with version
   â”œâ”€ Pushes to ECR (manually, easy to forget)
   â”œâ”€ Manually updates Kubernetes manifests
   â”œâ”€ Applies kubectl apply (hoping nothing breaks)
   â”œâ”€ Watches logs to see if it works
   â””â”€ Rolls back if broken (manually)
3. Risk: Human error at every step
4. Time: 30 minutes per deployment
5. Consistency: Different every time

âœ… AUTOMATED CI/CD PIPELINE (After GitHub Actions + ArgoCD):
1. Developer commits code to GitHub
2. GitHub Actions automatically:
   â”œâ”€ Builds Docker image
   â”œâ”€ Tags with commit SHA
   â”œâ”€ Pushes to ECR
   â”œâ”€ Updates values.yaml with new tag
   â”œâ”€ Commits to Git (automatic)
3. ArgoCD automatically:
   â”œâ”€ Detects Git change
   â”œâ”€ Pulls updated values.yaml
   â”œâ”€ Deploys via Helm
   â”œâ”€ Monitors health
   â”œâ”€ Rolls back if unhealthy (auto-remediation)
4. Result: Deployment in 2 minutes, zero human error
5. Complete audit trail: who changed what, when
```

### CI/CD Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COMPLETE CI/CD FLOW                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 1: DEVELOPER PUSHES CODE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Developer  â”‚
â”‚   git push   â”‚ (to feature branch)
â”‚   (new code) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ (to GitHub)
       â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  GitHub Repository                            â”‚
   â”‚  â”œâ”€ main branch (source of truth)             â”‚
   â”‚  â””â”€ .github/workflows/build-push-ui.yaml      â”‚
   â”‚     (CI pipeline definition)                  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 2: GITHUB ACTIONS (Continuous Integration)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
GitHub Actions Workflow Triggered:

1. CHECKOUT (Clone repository)
   â”œâ”€ Action: actions/checkout
   â””â”€ Result: Code in runner

2. AUTHENTICATE TO AWS (OIDC - NO SECRETS!)
   â”œâ”€ Action: aws-actions/configure-aws-credentials
   â”œâ”€ Flow:
   â”‚  â”œâ”€ GitHub generates OIDC token (tied to this action)
   â”‚  â”œâ”€ Calls AWS STS: ExchangeWebIdentityForToken
   â”‚  â”œâ”€ AWS verifies token (GitHub is trusted)
   â”‚  â”œâ”€ Returns temporary AWS credentials (valid 1 hour)
   â”œâ”€ Result: Secure AWS access
   â””â”€ NO HARDCODED AWS_ACCESS_KEY_ID! âœ…

3. LOGIN TO ECR (Amazon Elastic Container Registry)
   â”œâ”€ Action: aws-actions/amazon-ecr-login
   â”œâ”€ Retrieves ECR credentials
   â””â”€ Result: Can push images to ECR

4. BUILD DOCKER IMAGE
   â”œâ”€ Action: docker/build-push-action
   â”œâ”€ Dockerfile: backend/ui/Dockerfile
   â”œâ”€ Build context: ./ (entire repo)
   â”œâ”€ Base image: node:18-alpine
   â”œâ”€ Compiles TypeScript/React
   â”œâ”€ Bundles assets
   â””â”€ Result: Docker image layer built

5. TAG IMAGE
   â”œâ”€ Tag 1: 123456789.dkr.ecr.us-east-1.amazonaws.com/retail-store/ui:latest
   â”œâ”€ Tag 2: 123456789.dkr.ecr.us-east-1.amazonaws.com/retail-store/ui:sha-abc1234
   â”‚         (short git commit SHA for traceability)
   â””â”€ Result: Image tagged with version

6. PUSH TO ECR
   â”œâ”€ Action: docker/build-push-action (with push: true)
   â”œâ”€ Target: Amazon ECR repository
   â”œâ”€ Encryption: KMS-encrypted images
   â””â”€ Result: Image available in ECR

7. UPDATE HELM VALUES
   â”œâ”€ File: 03_RetailStore_Helm_with_Data_Plane/values-ui.yaml
   â”œâ”€ Change: image.tag from "v1.0.0" to "sha-abc1234"
   â”œâ”€ Commit: auto-commit to main branch (via GitHub Actions)
   â””â”€ Result: Git updated with new image tag

8. WORKFLOW COMPLETE
   â””â”€ Status: CI pipeline success âœ…

STEP 3: GIT CHANGE DETECTED
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
values-ui.yaml updated:
â”œâ”€ Old: image.tag: "v1.0.0"
â””â”€ New: image.tag: "sha-abc1234"

ArgoCD continuously watches this file (every 30 seconds)

STEP 4: ARGOCD (Continuous Deployment - GitOps)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ArgoCD detects change:

1. SYNC DETECTION
   â”œâ”€ ArgoCD compares: Git state vs Cluster state
   â”œâ”€ Finds difference: values-ui.yaml changed
   â””â”€ Status: OUT OF SYNC

2. AUTOMATIC SYNC (if configured)
   â”œâ”€ Trigger: Auto-sync enabled in Application CRD
   â”œâ”€ Action: Deploy latest changes
   â””â”€ Policy: Automatic, Prune, Self-heal

3. HELM UPGRADE
   â”œâ”€ Action: helm upgrade ui ./charts/ui -f values-ui.yaml
   â”œâ”€ Reads: values-ui.yaml (with new image.tag)
   â”œâ”€ Generates: Complete K8s manifests from Helm
   â””â”€ Result: Kubernetes manifests ready

4. DEPLOYMENT STRATEGY
   â”œâ”€ Type: RollingUpdate
   â”œâ”€ Current: 3 pods running old version (v1.0.0)
   â”‚
   â””â”€ Steps:
      â”œâ”€ Create new pod with sha-abc1234
      â”œâ”€ Wait for readiness probe (healthy)
      â”œâ”€ Route traffic to new pod (via service)
      â”œâ”€ Terminate old pod
      â”œâ”€ Repeat for pod 2
      â”œâ”€ Repeat for pod 3
      â””â”€ ZERO-DOWNTIME! âœ… (always 2-3 pods running)

5. MONITORING & VERIFICATION
   â”œâ”€ ArgoCD monitors rollout progress
   â”œâ”€ Checks pod health
   â”œâ”€ Verifies readiness probes pass
   â””â”€ Status: SYNCED (successful)

STEP 5: APPLICATION RUNNING (NEW VERSION)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â”œâ”€ UI pod 1: running new version sha-abc1234
â”œâ”€ UI pod 2: running new version sha-abc1234
â”œâ”€ UI pod 3: running new version sha-abc1234
â”œâ”€ Users accessing: New feature deployed!
â””â”€ Deployment time: ~2 minutes âœ…

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
COMPLETE FLOW: Code â†’ Pushed â†’ GitHub Actions â†’ ECR â†’ ArgoCD â†’ EKS
Time: Code push to live: ~2-3 minutes âœ… Fully automated âœ…
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### GitHub Actions Workflow

**CI Pipeline: Build, Test, Push â†’ ECR:**

```
Developer: git push origin main
            â”‚
            â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ GitHub Repository Webhook       â”‚
        â”‚ (push detected)                  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ GitHub Actions Runner              â”‚
    â”‚ (ubuntu-latest machine)            â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â†“             â†“          â†“          â†“               â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Checkout â”‚ â”‚ Configureâ”‚ â”‚ Login to â”‚ â”‚ Build &  â”‚ â”‚ Push to ECR  â”‚
    â”‚ Code     â”‚ â”‚ AWS      â”‚ â”‚ ECR      â”‚ â”‚ Tag      â”‚ â”‚              â”‚
    â”‚ from Git â”‚ â”‚ (OIDC)   â”‚ â”‚ (Temp    â”‚ â”‚ Docker   â”‚ â”‚ New image   â”‚
    â”‚          â”‚ â”‚          â”‚ â”‚ creds)   â”‚ â”‚ image    â”‚ â”‚ versions up  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚              â”‚
                      â”‚            â”‚            â”‚         â”‚              â”‚
                      â†“            â”‚            â”‚         â”‚              â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚            â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚ OIDC Token + STSâ”‚    â”‚            â”‚                  â”‚
            â”‚ No hardcoded    â”‚    â”‚            â”‚                  â†“
            â”‚ credentials! âœ… â”‚    â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚                       â”‚             â”‚
                                   â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”
                                   â”‚        â†“              â†“              â†“
                                   â”‚      Image:latest  Image:sha-abc123  Image tags
                                   â”‚      Image updated in ECR repository
                                   â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚ Step: Update Helm Values                  â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â†“
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚ values-ui.yaml updated:      â”‚
                     â”‚ image.tag: sha-abc123        â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                       â”‚
                    â†“                       â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Git Commit          â”‚  â”‚ Push to GitHub      â”‚
        â”‚ "chore: Update UI   â”‚  â”‚ (automatic)         â”‚
        â”‚  image tag to       â”‚  â”‚                     â”‚
        â”‚  sha-abc123"        â”‚  â”‚ values-ui.yaml      â”‚
        â”‚                     â”‚  â”‚ committed to main   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â†“
                                 ArgoCD watches Git...
```

```yaml
# .github/workflows/build-push-ui.yaml

name: Build and Push UI to ECR

on:
  push:
    branches:
      - main
      - develop
    paths:
      - 'ui/**'              # Only trigger on UI changes
      - '.github/workflows/build-push-ui.yaml'

env:
  AWS_REGION: us-east-1
  ECR_REPOSITORY: retail-store/ui
  IMAGE_TAG: ${{ github.sha }}  # Git commit SHA

jobs:
  build-and-push:
    runs-on: ubuntu-latest
    
    permissions:
      id-token: write        # OIDC token permission
      contents: read
    
    steps:
    # Step 1: Checkout code
    - name: Checkout Code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0       # Full history for version detection
    
    # Step 2: Configure AWS credentials using OIDC
    - name: Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        role-to-assume: arn:aws:iam::123456789:role/github-actions-role
        aws-region: ${{ env.AWS_REGION }}
        role-skip-session-tagging: true
      # Behind the scenes:
      # 1. GitHub generates OIDC token (tied to repo, branch, action)
      # 2. Calls AWS STS: AssumeRoleWithWebIdentity
      # 3. AWS verifies GitHub's OIDC provider signature
      # 4. Returns temporary credentials (1 hour)
      # 5. Sets as environment variables
      # NO HARDCODED AWS KEYS! âœ…
    
    # Step 3: Login to Amazon ECR
    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2
      with:
        mask-password: true  # Don't expose password in logs
    
    - name: Get ECR Registry
      id: ecr-uri
      run: echo "registry=${{ steps.login-ecr.outputs.registry }}" >> $GITHUB_OUTPUT
    
    # Step 4: Build and Push to ECR
    - name: Build and Push Docker Image
      uses: docker/build-push-action@v5
      with:
        context: ./ui              # Dockerfile location
        push: true                 # Push to ECR
        
        # Image tagging
        tags: |
          ${{ steps.ecr-uri.outputs.registry }}/${{ env.ECR_REPOSITORY }}:latest
          ${{ steps.ecr-uri.outputs.registry }}/${{ env.ECR_REPOSITORY }}:${{ env.IMAGE_TAG }}
        
        # Build arguments
        build-args: |
          REACT_APP_API_ENDPOINT=https://api.example.com
          BUILD_DATE=${{ github.event.head_commit.timestamp }}
          VCS_REF=${{ github.sha }}
        
        # Caching for faster builds
        cache-from: type=gha
        cache-to: type=gha,mode=max
    
    # Step 5: Update values.yaml in Git
    - name: Update Helm Values
      run: |
        # Update image tag in values-ui.yaml
        sed -i "s|tag: .*|tag: ${{ env.IMAGE_TAG }}|" \
            03_RetailStore_Helm_with_Data_Plane/02_retailstore_values_HELM_aws_dataplane/values-ui.yaml
        
        # Show what changed
        cat 03_RetailStore_Helm_with_Data_Plane/02_retailstore_values_HELM_aws_dataplane/values-ui.yaml
    
    # Step 6: Commit and push changes
    - name: Commit and Push Changes
      run: |
        git config --global user.email "github-actions@github.com"
        git config --global user.name "GitHub Actions"
        
        # Stage changes
        git add 03_RetailStore_Helm_with_Data_Plane/02_retailstore_values_HELM_aws_dataplane/values-ui.yaml
        
        # Check if there are changes
        if git diff --cached --quiet; then
          echo "No changes to commit"
        else
          git commit -m "chore: Update UI image tag to ${{ env.IMAGE_TAG }}"
          git push origin main
        fi
    
    # Step 7: Notify Slack (optional)
    - name: Notify Slack
      if: always()
      uses: slackapi/slack-github-action@v1
      with:
        payload: |
          {
            "text": "UI Deployment: ${{ job.status }}",
            "blocks": [
              {
                "type": "section",
                "text": {
                  "type": "mrkdwn",
                  "text": "UI Docker Image Build\nStatus: ${{ job.status }}\nCommit: ${{ github.sha }}\nImage: retail-store/ui:${{ env.IMAGE_TAG }}"
                }
              }
            ]
          }
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}
```

### OIDC Authentication: Zero-Secrets Workflow

**How GitHub Actions Gets AWS Credentials Without Storing Secrets:**

```
Step 1: GITHUB ACTIONS WORKFLOW STARTS
â”œâ”€ Job: build-and-push (on: push to main branch)
â”œâ”€ Runner: ubuntu-latest
â”œâ”€ Environment: Workflow execution context
â”‚  â”œâ”€ Repository: myorg/retail-store
â”‚  â”œâ”€ Branch: main
â”‚  â”œâ”€ Actor: developer-name
â”‚  â”œâ”€ Commit SHA: abc1234567890def
â”‚  â””â”€ Timestamp: 2024-02-09T10:15:32Z
â”‚
â””â”€ No AWS credentials in environment âœ…

Step 2: REQUEST OIDC TOKEN
â”œâ”€ Action: actions/configure-aws-credentials
â”œâ”€ Step: "id-token: write"
â”‚  â””â”€ Permission to request OIDC token from GitHub
â”‚
â”œâ”€ GitHub generates token:
â”‚  â”œâ”€ Type: JWT (JSON Web Token)
â”‚  â”œâ”€ Signed with: GitHub's private key (kept secure)
â”‚  â””â”€ Content:
â”‚     â”œâ”€ github_repository: myorg/retail-store
â”‚     â”œâ”€ github_ref: main
â”‚     â”œâ”€ github_actor: developer-name
â”‚     â”œâ”€ github_job: build-and-push
â”‚     â”œâ”€ github_sha: abc1234567890def
â”‚     â”œâ”€ iss: https://token.actions.githubusercontent.com
â”‚     â”œâ”€ aud: sts.amazonaws.com
â”‚     â”œâ”€ iat: 1707467732 (issued at)
â”‚     â””â”€ exp: 1707467792 (expires in 1 hour)
â”‚
â””â”€ Token ready âœ…

Step 3: EXCHANGE TOKEN FOR AWS CREDENTIALS
â”œâ”€ Action calls: aws sts assume-role-with-web-identity
â”‚  â”œâ”€ WebIdentityToken: JWT from step 2
â”‚  â”œâ”€ RoleArn: arn:aws:iam::123456789:role/github-actions-role
â”‚  â””â”€ RoleSessionName: myorg-retail-store-abc1234
â”‚
â”œâ”€ AWS processes request:
â”‚  â”œâ”€ Find: github-actions-role (in IAM)
â”‚  â”œâ”€ Check: Trust relationship
â”‚  â”‚  â””â”€ "Allowed principals: OIDC provider, GitHub"
â”‚  â”‚
â”‚  â”œâ”€ Verify: OIDC token signature
â”‚  â”‚  â”œâ”€ Get GitHub's public key (cached)
â”‚  â”‚  â”œâ”€ Verify: JWT signature is valid âœ“
â”‚  â”‚  â”œâ”€ Verify: Token not expired âœ“
â”‚  â”‚  â””â”€ Verified: Truly from GitHub âœ…
â”‚  â”‚
â”‚  â”œâ”€ Check: Token claim conditions
â”‚  â”‚  â”œâ”€ Condition: aud == "sts.amazonaws.com" âœ“
â”‚  â”‚  â”œâ”€ Condition: sub (subject) matches policy pattern
â”‚  â”‚  â”‚  â””â”€ Pattern: repo:myorg/retail-store:*
â”‚  â”‚  â”‚  â””â”€ Token subject: repo:myorg/retail-store:ref:refs/heads/main
â”‚  â”‚  â”‚  â””â”€ MATCHES âœ“
â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€ All conditions met âœ…
â”‚  â”‚
â”‚  â””â”€ APPROVED: This GitHub action is trusted

Step 4: GENERATE TEMPORARY STS CREDENTIALS
â”œâ”€ AWS STS creates temporary credentials:
â”‚  â”œâ”€ AccessKeyId: ASIAB5EXAMPLE2024
â”‚  â”‚  â””â”€ Temporary (only for this session)
â”‚  â”‚
â”‚  â”œâ”€ SecretAccessKey: wJalrXU...exampleKey/EXAMPLE
â”‚  â”‚  â””â”€ Temporary (only for this session)
â”‚  â”‚
â”‚  â”œâ”€ SessionToken: AQoDXw...exampleToken
â”‚  â”‚  â””â”€ Unique for this exchange
â”‚  â”‚
â”‚  â”œâ”€ Expiration: 3600 seconds (1 hour)
â”‚  â”‚  â””â”€ Credentials auto-expire
â”‚  â”‚  â””â”€ Can't be reused after expiration
â”‚  â”‚
â”‚  â””â”€ Policy: github-actions-role permissions
â”‚     â”œâ”€ ecr:GetAuthorizationToken
â”‚     â”œâ”€ ecr:BatchGetImage
â”‚     â”œâ”€ ecr:InitiateLayerUpload
â”‚     â”œâ”€ ecr:CompleteLayerUpload
â”‚     â”œâ”€ ecr:PutImage
â”‚     â””â”€ (limited scope, not full AWS access)

â””â”€ Credentials returned to GitHub Actions âœ…

Step 5: SET ENVIRONMENT VARIABLES
â”œâ”€ GitHub Actions saves credentials as env vars:
â”‚  â”œâ”€ AWS_ACCESS_KEY_ID=ASIAB5EXAMPLE2024
â”‚  â”œâ”€ AWS_SECRET_ACCESS_KEY=wJalrXU...exampleKey/EXAMPLE
â”‚  â””â”€ AWS_SESSION_TOKEN=AQoDXw...exampleToken
â”‚
â”œâ”€ Available to: Job steps only
â”œâ”€ NOT visible in: Logs (masked by GitHub Actions)
â”œâ”€ NOT persisted: Deleted after workflow completion
â””â”€ Security: Temporary credentials âœ…

Step 6: AUTHENTICATE TO ECR
â”œâ”€ Step: "aws-actions/amazon-ecr-login"
â”œâ”€ Uses: AWS credentials from above
â”œâ”€ Call: aws ecr get-authorization-token
â”‚  â”œâ”€ Authenticates with temporary credentials
â”‚  â”œâ”€ Validates: credentials are valid âœ“
â”‚  â””â”€ Role has: ecr:GetAuthorizationToken âœ“
â”‚
â”œâ”€ Response: ECR login token
â”‚  â”œâ”€ Type: Docker authentication token
â”‚  â”œâ”€ Expires: 12 hours
â”‚  â””â”€ Allows: Push to ECR
â”‚
â””â”€ Logged in to ECR âœ…

Step 7: DOCKER BUILD & PUSH
â”œâ”€ Step: "docker/build-push-action"
â”œâ”€ Uses: ECR login token from above
â”œâ”€ Build Docker image
â”œâ”€ Tag image with ECR URI
â”œâ”€ Push to Amazon ECR
â”‚  â”œâ”€ Each layer encrypted in transit
â”‚  â”œâ”€ Each layer encrypted at rest (KMS)
â”‚  â””â”€ Audit logged in CloudTrail
â”‚
â””â”€ Image pushed âœ…

Step 8: CREDENTIALS EXPIRE
â”œâ”€ Time elapsed: 59 minutes
â”œâ”€ Temporary credentials: 1 minute remaining
â”œâ”€ Workflow status: COMPLETE
â”‚  â””â”€ Temporary credentials: Revoked automatically
â”‚
â”œâ”€ What happens to stolen creds?
â”‚  â”œâ”€ If leaked after job: Expired (can't use)
â”‚  â”œâ”€ Validity: 1 hour only
â”‚  â”œâ”€ Scope: Limited (ECR only, not full AWS)
â”‚  â””â”€ Audit trail: CloudTrail logs who accessed what
â”‚
â””â”€ Security: Minimal blast radius âœ…

SECURITY ADVANTAGES
â”œâ”€ âŒ Before (Leaked AWS Access Key):
â”‚  â”œâ”€ Permanent key stored as GitHub Secret
â”‚  â”œâ”€ Valid forever (until manual rotation)
â”‚  â”œâ”€ Full AWS permissions (attacker gets everything)
â”‚  â”œâ”€ Manual rotation required (tedious)
â”‚  â””â”€ Blast radius: Unlimited
â”‚
â””â”€ âœ… After (OIDC Token):
   â”œâ”€ Token valid 5 minutes only
   â”œâ”€ Temporary credentials valid 1 hour only
   â”œâ”€ Limited permissions (least privilege)
   â”œâ”€ Auto-expiration (no manual rotation needed)
   â”œâ”€ Fine-grained audit trail (repo, branch, workflow, commit)
   â””â”€ Blast radius: Minimal
```

### OIDC Trust Policy: GitHub â†’ AWS

```hcl
# Trust policy allowing GitHub Actions to assume role

data "aws_iam_policy_document" "github_actions_trust" {
  statement {
    sid     = "AllowGitHubActionsOIDC"
    effect  = "Allow"
    actions = ["sts:AssumeRoleWithWebIdentity"]
    
    principals {
      type        = "Federated"
      identifiers = [aws_iam_openid_connect_provider.github.arn]
    }
    
    # Only allow tokens from specific repository
    condition {
      test     = "StringEquals"
      variable = "token.actions.githubusercontent.com:aud"
      values   = ["sts.amazonaws.com"]
    }
    
    condition {
      test     = "StringLike"
      variable = "token.actions.githubusercontent.com:sub"
      values   = ["repo:myorg/retail-store:*"]  # Your GitHub repo
    }
  }
}

resource "aws_iam_role" "github_actions" {
  name               = "github-actions-role"
  assume_role_policy = data.aws_iam_policy_document.github_actions_trust.json
}

# Minimal permissions: only what GitHub Actions needs
data "aws_iam_policy_document" "github_actions" {
  statement {
    sid    = "ECRAccess"
    effect = "Allow"
    actions = [
      "ecr:GetAuthorizationToken",          # Login to ECR
      "ecr:BatchGetImage",
      "ecr:GetDownloadUrlForLayer",
      "ecr:DescribeImages",
      "ecr:DescribeRepositories",
      "ecr:ListImages",
      "ecr:InitiateLayerUpload",
      "ecr:UploadLayerPart",
      "ecr:CompleteLayerUpload",
      "ecr:PutImage"                        # Push to ECR
    ]
    resources = [
      "arn:aws:ecr:us-east-1:123456789:repository/retail-store/*"
    ]
  }
}

resource "aws_iam_role_policy" "github_actions" {
  name   = "github-actions-policy"
  role   = aws_iam_role.github_actions.id
  policy = data.aws_iam_policy_document.github_actions.json
}
```

### ArgoCD Setup & Configuration

**Step-by-Step: GitOps Sync Cycle (Complete Deployment Automation)**

```
STEP 1: DEVELOPER PUSHES CODE & TAGS IMAGE
â””â”€ Feature complete: feature/ui-redesign â†’ main branch
â””â”€ GitHub Actions CI completes: Docker build, image scan, ECR push
â””â”€ Image tag: 123456789.dkr.ecr.us-east-1.amazonaws.com/ui:sha-abc123def
â””â”€ Helm values repository updated: infrastructure-as-code/retail-ui/values-prod.yaml
   â””â”€ Change: image.tag: "sha-1.0.0" â†’ "sha-abc123def"
   â””â”€ Commit message: "chore: bump UI image to sha-abc123def"
   â””â”€ Push to GitOps repo (separate from app code)

STEP 2: DEVELOPERS SLEEP ðŸ˜´, GITOPS AUTOMATION WAKES UP
â””â”€ ArgoCD Application resource watches: "argocd.example.com/argocd-server"
â””â”€ Application manifest points to:
   â””â”€ Git repository: git@github.com:company/retail-infrastructure.git
   â””â”€ Path: /12_Helm/retail-ui/
   â””â”€ Values file: values-prod.yaml
   â””â”€ Sync policy: automatic (enabled)
   â””â”€ Sync interval: every 180 seconds (3 min) OR webhook-triggered
â””â”€ Previous state cached in ArgoCD database (etcd)
   â””â”€ Last known Git commit: sha-xyz789
   â””â”€ Last known K8s deployment state: 3 pods with v1.0.0

STEP 3: ARGOCD CONTROLLER DETECTS CHANGE
â””â”€ Push event webhook OR polling interval reached
â””â”€ API call to GitHub: GET /repos/company/retail-infrastructure/contents/12_Helm/retail-ui/values-prod.yaml
â””â”€ Response includes new commit: sha-abc123def
â””â”€ Comparison: SHA-ABC123DEF (Git) vs SHA-XYZ789 (cached) â†’ DIFFERENT âŒ
â””â”€ Result: Application marked "OutOfSync"
   â””â”€ Reason: Git state â‰  Cluster state
   â””â”€ ArgoCD UI dashboard shows ðŸ”´ SYNCING indicator

STEP 4: ARGOCD FETCHES GIT MANIFESTS & RENDERS TEMPLATES
â””â”€ Git clone: release-notes/retail-infrastructure to /tmp/argocd-cache-12345
â””â”€ Checkout commit: sha-abc123def
â””â”€ Helm template rendering:
   â”œâ”€ Input: chart path = /12_Helm/retail-ui/
   â”œâ”€ Input: values file = /12_Helm/retail-ui/values-prod.yaml
      â”‚  â””â”€ image.tag: "sha-abc123def"
      â”‚  â””â”€ image.repository: "123456789.dkr.ecr.us-east-1.amazonaws.com/ui"
      â”‚  â””â”€ replicas: 3
      â”‚  â””â”€ resources.requests.memory: "256Mi"
      â”‚  â””â”€ hpa.minReplicas: 2, hpa.maxReplicas: 10
   â”œâ”€ Helm engine processes template files:
      â”‚  â”œâ”€ deployment.yaml: manifest â†’ {{ include "ui.deployment" . }} rendered
      â”‚  â”œâ”€ service.yaml: ClusterIP service â†’ cluster.local:8080
      â”‚  â”œâ”€ hpa.yaml: metrics-based scaling configured
      â”‚  â”œâ”€ servicemonitor.yaml: Prometheus monitoring setup
      â”‚  â””â”€ ingress.yaml: ALB routing rules â†’ http://ui.retail.aws.company.com
   â”œâ”€ Output: rendered YAML manifests (no templating syntax left)
   â””â”€ Git diff calculated: what changed since last sync?
      â””â”€ Changed: Deployment.spec.template.spec.containers[0].image
         â””â”€ FROM: 123456789.dkr.ecr.us-east-1.amazonaws.com/ui:sha-1.0.0
         â””â”€ TO:   123456789.dkr.ecr.us-east-1.amazonaws.com/ui:sha-abc123def
      â””â”€ Unchanged: replicas, resources, service, HPA, ingress

STEP 5: ARGOCD COMPARES GIT STATE (DESIRED) vs CLUSTER STATE (CURRENT)
â””â”€ Query Kubernetes API: kubectl get all -n retail-products
â””â”€ Existing state:
   â”œâ”€ Deployment.ui: 3 pods running
   â”‚  â”œâ”€ Pod 1 (ui-5c9f7b2d1-xyz): image sha-1.0.0, running 47 minutes
   â”‚  â”œâ”€ Pod 2 (ui-5c9f7b2d1-abc): image sha-1.0.0, running 43 minutes
   â”‚  â””â”€ Pod 3 (ui-5c9f7b2d1-def): image sha-1.0.0, running 39 minutes
   â”œâ”€ Service.ui: ClusterIP 10.100.50.42 with endpoints [10.0.11.45, 10.0.12.46, 10.0.13.47]
   â”œâ”€ HPA: currently 3 replicas (metrics: CPU 42%, Memory 51%)
   â””â”€ Ingress: active, routing traffic to Service.ui
â””â”€ Desired state (from Git/Helm templates):
   â”œâ”€ Deployment.ui: 3 pods running
   â”‚  â””â”€ Image: sha-abc123def (THE CHANGE)
   â”œâ”€ Service.ui: same ClusterIP, same endpoints after rollout
   â”œâ”€ HPA: same configuration
   â””â”€ Ingress: same routing
â””â”€ Comparison result: DIFFERENT âŒ
   â””â”€ Required action: RollingUpdate deployment with new image
   â””â”€ Safety check: RollingUpdateStrategy confirmed (maxSurge: 1, maxUnavailable: 0)
      â””â”€ Ensures zero downtime: never go below 3 pods, never exceed 4 pods

STEP 6: ARGOCD EXECUTES SYNC (CREATE/UPDATE/DELETE OPERATIONS)
â””â”€ Sync mode: automatic + self-healing enabled
â””â”€ Operations ordered by ArgoCD:
   â”œâ”€ Operation 1: PATCH Deployment.spec.template.spec.containers[0].image
      â”‚  â””â”€ Kubernetes API call: kubectl patch deployment ui ...
      â”‚  â””â”€ Effect: triggers RollingUpdate deployment controller
      â”‚  â””â”€ Duration: 1-3 seconds for API response
      â”‚
      â”œâ”€ Kubernetes RollingUpdate Orchestration (automatic, initiated by patch):
      â”‚  â”œâ”€ Step A: Create pod 1 (new) - image sha-abc123def
      â”‚  â”‚  â””â”€ Kubelet on node-1 pulls image: sha-abc123def
      â”‚  â”‚  â””â”€ Container starts, application initializes (5-15 seconds)
      â”‚  â”‚  â””â”€ Readiness probe: GET /ready â†’ connected to DB â†’ 200 OK âœ…
      â”‚  â”‚
      â”‚  â”œâ”€ Step B: Update Service.ui endpoints
      â”‚  â”‚  â””â”€ Service controller adds new pod to endpoints: 10.0.11.48
      â”‚  â”‚  â””â”€ Load balancer (kube-proxy) starts routing some traffic to new pod
      â”‚  â”‚  â””â”€ New pod receives traffic (gradually, depends on readiness)
      â”‚  â”‚
      â”‚  â”œâ”€ Step C: Terminate pod 1 (old) - graceful shutdown
      â”‚  â”‚  â””â”€ Kubelet sends SIGTERM to container (preStop hook: 30 sec to drain connections)
      â”‚  â”‚  â””â”€ Load balancer stops routing traffic (endpoint removed)
      â”‚  â”‚  â””â”€ Pod terminates after 30 seconds, freeing resources
      â”‚  â”‚
      â”‚  â”œâ”€ Step D: Repeat for pod 2 (old)
      â”‚  â”‚  â””â”€ Create pod 2 (new) â†’ verify readiness â†’ update endpoints â†’ terminate old
      â”‚  â”‚
      â”‚  â””â”€ Step E: Repeat for pod 3 (old)
      â”‚     â””â”€ Create pod 3 (new) â†’ verify readiness â†’ update endpoints â†’ terminate old
      â”‚
      â””â”€ Operation Complete: All 3 pods running with sha-abc123def
         â””â”€ New pods: 10.0.11.48, 10.0.12.47, 10.0.13.48
         â””â”€ Endpoints updated in Service: [10.0.11.48, 10.0.12.47, 10.0.13.48]
         â””â”€ HTTP traffic: Users still connected to ui.retail.aws.company.com âœ…
         â””â”€ Zero downtime: never dropped active connections
         â””â”€ Complete time: 2-5 minutes (depending on image size, startup time)

STEP 7: ARGOCD HEALTH CHECKS & MONITORING
â””â”€ After sync complete, ArgoCD monitors application health:
   â”œâ”€ Kubernetes resource status:
   â”‚  â”œâ”€ Deployment.ui healthy?
   â”‚  â”‚  â””â”€ Desired: 3, Current: 3, Ready: 3, Updated: 3 âœ…
   â”‚  â”œâ”€ Service.ui healthy?
   â”‚  â”‚  â””â”€ Endpoints: 3 active, all passing readiness probe âœ…
   â”‚  â””â”€ HPA healthy?
   â”‚     â””â”€ Metrics collection working, scale decisions ready âœ…
   â”‚
   â”œâ”€ Pod-level health:
   â”‚  â””â”€ Readiness probes: all green âœ…
   â”‚  â””â”€ Liveness probes: all responding âœ…
   â”‚  â””â”€ Resource limits: memory 256Mi/pod Ã— 3 = 768Mi total âœ…
   â”‚
   â””â”€ Application health (custom):
      â””â”€ HTTP endpoint: GET /health â†’ 200 OK âœ…
      â””â”€ Database connectivity: connection pool â†’ MySQL âœ…
      â””â”€ Service mesh injection: Istio sidecar running (if enabled) âœ…

STEP 8: ARGOCD STATUS UPDATE & AUDIT LOG
â””â”€ Application status updated:
   â”œâ”€ Status.conditions[0].type: Synced
   â”œâ”€ Status.health.status: Healthy
   â”œâ”€ Status.operationState.finishedAt: 2024-01-15T10:34:22Z
   â”œâ”€ Status.operationState.syncResult.resources: [Deployment, Service, HPA, Ingress] updated
   â””â”€ Message: "successfully synced (all resources: 4 synced, 0 failed)"
â””â”€ Git audit trail (automatic):
   â”œâ”€ Pushed by: developer-name
   â”œâ”€ Commit message: "chore: bump UI image to sha-abc123def"
   â”œâ”€ Deployment time: 2024-01-15 10:34:22 UTC
   â”œâ”€ ArgoCD sync timestamp: logged with Git commit SHA
   â””â”€ Rollback-safe: can instant revert to sha-1.0.0 by reverting Git commit
â””â”€ Dashboard shows:
   â”œâ”€ Application.ui: ðŸŸ¢ SYNCED, ðŸŸ¢ HEALTHY
   â”œâ”€ Last sync: 2 seconds ago
   â”œâ”€ Sync log: 4 resources updated successfully
   â””â”€ Git commit: sha-abc123def by developer-name
â””â”€ Notifications (if configured):
   â”œâ”€ Slack: "@dev-team UI deployment complete (sha-abc123def)"
   â”œâ”€ Email: "ui application synced in 3 minutes"
   â””â”€ PagerDuty: no alert (all healthy)

STEP 9: CONTINUOUS SELF-HEALING (ONGOING)
â””â”€ ArgoCD continues monitoring every 180 seconds:
   â”œâ”€ If pod crashes â†’ new pod auto-created (Deployment controller)
   â”œâ”€ If developer manually deletes pod â†’ Deployment controller recreates
   â”œâ”€ If someone manually edits Deployment in cluster (kubectl edit) â†’ ArgoCD reverts
   â”‚  â””â”€ Reason: Git is source of truth, not cluster state
   â”‚  â””â”€ Message: "OutOfSync" â†’ auto-revert to Git state
   â””â”€ Result: Application always matches Git state (GitOps guarantee)

TOTAL DEPLOYMENT TIME: ~2-5 minutes (from Git push to all pods running)
ZERO DOWNTIME: âœ… RollingUpdate keeps service active
ROLLBACK TIME: ~1 minute (git revert + ArgoCD auto-sync)
AUDIT TRAIL: âœ… Complete Git history (who, what, when, why)
```

**CD Pipeline: GitOps Synchronization â†’ Kubernetes:**

```
Git Change Detected (values-ui.yaml updated with new tag)
       â”‚
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Git Repository (Source of Truth)                     â”‚
â”‚ â”œâ”€ values-ui.yaml: image.tag = sha-abc123          â”‚
â”‚ â”œâ”€ deployment.yaml: template with {{ values }}     â”‚
â”‚ â”œâ”€ service.yaml, hpa.yaml, etc.                    â”‚
â”‚ â””â”€ Helm charts: /03_RetailStore_Helm_...           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
        Every 30 seconds (or webhook)
                 â”‚
                 â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ ArgoCD Controller                   â”‚
  â”‚ (argocd-application-controller)     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
             Poll Git Repo
                   â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â†“                   â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚Git Stateâ”‚        â”‚Cluster State â”‚
    â”‚(in Git) â”‚        â”‚(in K8s)      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
         Are they the same?
                   â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“                     â†“
      YES                     NO
      (Synced)           (Out of Sync)
        â”‚                     â”‚
        â””â”€        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚                    â”‚
                  â†“                    â†“
            Auto-Sync ON          Auto-Sync OFF
            (configured)          (manual only)
                  â”‚                    â”‚
                  â””â”€ Helm upgrade â”€â”€â”€â”€â”€â”´â”€ Alert developer
                        â”‚
                        â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ helm upgrade ui ./charts/ui   â”‚
        â”‚   -f values-ui.yaml             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Kubernetes Deployment Rollout   â”‚
        â”‚                                 â”‚
        â”‚ Current: 3 pods (v1.0.0)       â”‚
        â”‚ Target:  3 pods (sha-abc123)   â”‚
        â”‚                                 â”‚
        â”‚ â”œâ”€ Create pod 1 (new)          â”‚
        â”‚ â”œâ”€ Wait for readiness          â”‚
        â”‚ â”œâ”€ Route traffic â†’ pod 1       â”‚
        â”‚ â”œâ”€ Terminate pod (old)         â”‚
        â”‚ â”œâ”€ Repeat for pod 2, 3        â”‚
        â”‚ â””â”€ ZERO DOWNTIME âœ…            â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ ArgoCD Monitoring               â”‚
        â”‚                                 â”‚
        â”‚ â”œâ”€ All pods healthy?            â”‚
        â”‚ â”œâ”€ Service endpoints ready?     â”‚
        â”‚ â”œâ”€ HPA configured correctly?    â”‚
        â”‚ â””â”€ Status: SYNCED âœ…            â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
              Application Running
              New version deployed
              Complete audit trail in Git
```

```yaml
---
# ArgoCD Namespace
apiVersion: v1
kind: Namespace
metadata:
  name: argocd

---
# Install ArgoCD via Helm (or AWS EKS Add-on)
# helm repo add argocd https://argoproj.github.io/argo-helm
# helm install argocd -n argocd --create-namespace argocd/argo-cd

---
# ArgoCD Application: UI Microservice
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: ui
  namespace: argocd
spec:
  # Project (RBAC grouping)
  project: default
  
  # Source: Where to deploy FROM (Git)
  source:
    repoURL: https://github.com/myorg/retail-store.git
    targetRevision: main                    # Branch to sync from
    
    # Using Helm as templating engine
    path: 03_RetailStore_Helm_with_Data_Plane/02_retailstore_values_HELM_aws_dataplane
    
    helm:
      releaseName: ui
      
      # Override default values
      values: |
        image:
          repository: 123456789.dkr.ecr.us-east-1.amazonaws.com/retail-store/ui
          tag: latest
      
      # Use custom values file
      valueFiles:
        - values-ui.yaml              # Primary values
  
  # Destination: Where to deploy TO (Kubernetes cluster)
  destination:
    server: https://kubernetes.default.svc  # Current cluster
    namespace: default                      # Target namespace
  
  # Sync Policy: How to keep cluster in sync
  syncPolicy:
    # Automatic sync: changes detected in Git â†’ auto-deploy
    automated:
      prune: true          # Delete resources not in Git
      selfHeal: true       # If pod crashes, redeploy
      allowEmpty: false    # Don't delete all pods
    
    # Progressive syncing
    syncOptions:
    - CreateNamespace=true
    - RespectIgnoreDifferences=true
    
    # Retry on failure
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
    
    # Health assessment
    progressDeadlineSeconds: 600
  
  # Revision History (keep last 10)
  revisionHistoryLimit: 10

---
# ArgoCD Application: Catalog Service
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: catalog
  namespace: argocd
spec:
  project: default
  
  source:
    repoURL: https://github.com/myorg/retail-store.git
    targetRevision: main
    path: 03_RetailStore_Helm_with_Data_Plane/02_retailstore_values_HELM_aws_dataplane
    
    helm:
      releaseName: catalog
      valueFiles:
        - values-catalog.yaml
  
  destination:
    server: https://kubernetes.default.svc
    namespace: default
  
  syncPolicy:
    automated:
      prune: true
      selfHeal: true

---
# Similar for other services (cart, checkout, orders)
# Each has its own Application CRD
```

### ArgoCD Management Commands

```bash
# Install ArgoCD
helm repo add argocd https://argoproj.github.io/argo-helm
helm install argocd argocd/argo-cd -n argocd --create-namespace

# Get initial admin password
kubectl -n argocd get secret argocd-initial-admin-secret \
  -o jsonpath="{.data.password}" | base64 -d

# Port forward to access UI
kubectl port-forward -n argocd svc/argocd-server 8080:443

# Access: https://localhost:8080
# Username: admin
# Password: (from above)

# CLI commands
# Login
argocd login localhost:8080

# Create application from CLI
argocd app create ui \
  --repo https://github.com/myorg/retail-store.git \
  --path 03_RetailStore_Helm_with_Data_Plane \
  --dest-server https://kubernetes.default.svc \
  --dest-namespace default

# View applications
argocd app list

# View specific app
argocd app get ui

# Sync (manual deployment)
argocd app sync ui

# Rollback to previous version
argocd app rollback ui 1

# Watch deployment
argocd app wait ui --sync

# Delete application
argocd app delete ui
```

---

## Complete Automation Workflow

```
COMPLETE END-TO-END AUTOMATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Developer Flow:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Developer makes code change (e.g., UI logo update)            â”‚
â”‚ 2. git commit && git push (to GitHub)                            â”‚
â”‚ 3. GitHub Actions triggered automatically:                      â”‚
â”‚    â”œâ”€ Build Docker image from code                              â”‚
â”‚    â”œâ”€ Tag with commit SHA                                       â”‚
â”‚    â”œâ”€ Push to Amazon ECR                                        â”‚
â”‚    â””â”€ Update Helm values.yaml with new image tag               â”‚
â”‚ 4. Git commit pushed (automatic)                                â”‚
â”‚    â””â”€ values-ui.yaml: image.tag = abc1234                      â”‚
â”‚ 5. ArgoCD detects change in Git:                               â”‚
â”‚    â”œâ”€ Reads updated values.yaml                                â”‚
â”‚    â”œâ”€ Helm charts generate K8s manifests                       â”‚
â”‚    â”œâ”€ Deploys to EKS via helm upgrade                          â”‚
â”‚    â””â”€ Rolling update (zero-downtime)                           â”‚
â”‚ 6. Application running with new changes                         â”‚
â”‚    â””â”€ End-to-end time: ~2-3 minutes                           â”‚
â”‚ 7. If something breaks:                                         â”‚
â”‚    â”œâ”€ ArgoCD detects unhealthy pods                           â”‚
â”‚    â”œâ”€ Auto-rollback to previous version                        â”‚
â”‚    â””â”€ Alerts developer (Slack, email)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

SYSTEM COMPONENTS WORKING TOGETHER:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GitHub Repo    â”‚ â† Source of truth for application code
â”‚  + Helm Charts  â”‚   + configuration
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    (webhook)
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GitHub Actions CI         â”‚ Builds & tests automatically
â”‚   (Build, Test, Push)       â”‚ on every push
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    (push image)
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Amazon ECR                 â”‚ Centralized container registry
â”‚   (Container Images)         â”‚ with encryption & scanning
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†‘
         â”‚  (pull image)
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ArgoCD                  â”‚ Continuous Deployment (GitOps)
â”‚   (GitOps Controller)     â”‚ watches Git for changes
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    (helm upgrade)
         â”‚
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   EKS Cluster                â”‚ Production environment
â”‚   (Running Pods)             â”‚ with Karpenter scaling
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Interview Q&A - Part 3

### Q1: "How does Karpenter improve upon traditional cluster autoscaling?"

**Answer**:
> "Karpenter is fundamentally different from Cluster Autoscaler:
>
> **Cluster Autoscaler (Traditional)**:
> - Watches Kubernetes Pending pods
> - Calls AWS Auto Scaling Groups API
> - ASG launches instances slowly (2-3 minutes)
> - Doesn't optimize instance selection
> - Struggles with Spot instances
> - Uses fixed node pools
>
> **Karpenter (Modern)**:
> - Watches Pending pods immediately
> - Calls EC2 FleetRequest API (parallel provisioning)
> - Launches instances in 15-30 seconds âœ…
> - Bin-packing: automatically selects right instance type
> - Native Spot support with interruption handling
> - Dynamic NodePools with consolidation
>
> **Key Improvements**:
>
> 1. **Speed**
>    - CA: 2-3 minutes (user timeout risk)
>    - Karpenter: 15-30 seconds (imperceptible delay)
>
> 2. **Cost Optimization**
>    - CA: Can't use Spot effectively
>    - Karpenter: 70% cheaper Spot instances with graceful handling
>    - Consolidation: Removes underutilized nodes
>    - Savings: 40-50% with mixed on-demand/Spot
>
> 3. **Instance Selection**
>    - CA: You specify instance types manually
>    - Karpenter: Analyzes pod requirements (CPU, memory), auto-selects optimal instance
>    - Example: 500m CPU request â†’ picks t3.medium (not t3.xlarge)
>
> 4. **Spot Interruption Handling**
>    - CA: No built-in support
>    - Karpenter: Monitors EventBridge for interruption notices
>    - Gracefully drains pods 2 minutes before interruption
>    - Consolidation replaces Spot nodes before expiration
>
> 5. **Zero-Downtime**
>    - Pods evicted gracefully with preStop hooks
>    - PDBs honored (keep minimum replicas)
>    - New pods scheduled before old pods terminate
>
> **Example Cost Savings**:
> - 100 pods needing 2vCPU each = 200vCPU total
> - Cluster Autoscaler: 20 on-demand t3.xlarge = \$2,429/month
> - Karpenter mixed: 15 on-demand + 5 Spot = \$1,001/month
> - **Savings: \$1,428/month (59%) âœ…**
>"

---

### Q2: "Walk me through a complete CI/CD deployment from code push to running app"

**Answer**:
> "It's a 5-step automated process:
>
> **Step 1: Developer Pushes Code (T=0)**
> - Developer: git push origin main
> - GitHub receives push
> - Webhook triggers GitHub Actions workflow
>
> **Step 2: GitHub Actions CI (T=0-1min)**
> - Checkout: Clone repo from GitHub
> - OIDC Auth: No hardcoded keys!
>   â”œâ”€ GitHub generates OIDC token
>   â”œâ”€ Token exchange with AWS STS
>   â””â”€ Get temporary credentials (1 hour validity)
> - Docker Build: Compile app, bundle assets
> - Tag Image: 
>   â”œâ”€ retail-store/ui:latest
>   â”œâ”€ retail-store/ui:abc1234 (commit SHA)
> - Push to ECR: Upload to Amazon ECR (encrypted)
> - Update Git: Modify values-ui.yaml with new image tag
> - Auto-commit: Push changes back to GitHub
>
> **Step 3: ArgoCD Detects Change (T=1-2min)**
> - ArgoCD continuously polls Git (every 30 seconds by default)
> - Detects: values-ui.yaml changed (image.tag updated)
> - Decision: Cluster state â‰  Git state â†’ OUT OF SYNC
> - Trigger: Auto-sync enabled â†’ DEPLOY
>
> **Step 4: ArgoCD Deploys via Helm (T=2-2.5min)**
> - Helm chart: 03_RetailStore_Helm_with_Data_Plane
> - Command: helm upgrade ui ./charts/ui -f values-ui.yaml
> - Process:
>   â”œâ”€ Render templates with new image tag
>   â”œâ”€ Generate Kubernetes manifest
>   â””â”€ Apply to cluster
>
> **Step 5: Kubernetes Executes Rolling Update (T=2.5-3min)**
> - Current state: 3 pods running old version (v1.0.0)
> - Rolling Update Strategy:
>   â”œâ”€ Create pod 1 with new version (abc1234)
>   â”œâ”€ Wait for readiness probe (HTTP /ready)
>   â”œâ”€ Service routes traffic to pod 1
>   â”œâ”€ Terminate old pod 1
>   â”œâ”€ Repeat for pod 2
>   â”œâ”€ Repeat for pod 3
>   â””â”€ Result: 3 pods new version, zero downtime âœ…
>
> **Step 6: Application Running (T=3min)**
> - All 3 UI pods running new version
> - ALB already routing traffic to them
> - Users see new feature immediately
> - Complete flow: code push â†’ live = **3 minutes**
>
> **Failure Handling**:
> - If pod fails readiness, Kubernetes doesn't proceed
> - ArgoCD detects unhealthy pods
> - Auto-rollback triggers (if configured)
> - Previous version restored
> - Alert sent (Slack)
>
> **Repeatability**:
> - No manual steps
> - Same process every time
> - Complete audit trail in Git
> - Can trace code change â†’ image tag â†’ deployment
>"

---

### Q3: "Why use OIDC instead of hardcoded AWS credentials for GitHub Actions?"

**Answer**:
> "It's a fundamental security improvement:
>
> **âŒ OLD WAY (Hardcoded Credentials)**:
> ```
> AWS_ACCESS_KEY_ID=AKIAI234567890ABC
> AWS_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY  
> ```
> - Stored as GitHub Secret (good first step)
> - But secrets are permanent (never expire)
> - If leaked: attacker has full AWS access
> - Difficult to rotate (must update everywhere)
> - Audit trail: Can't tell which action used credentials
>
> **âœ… NEW WAY (OIDC Web Identity)**:
> 1. GitHub generates OIDC token (signed by GitHub's private key)
> 2. Token contains metadata:
>    â”œâ”€ Repository: myorg/retail-store
>    â”œâ”€ Workflow: build-push-ui.yaml
>    â”œâ”€ Branch: main
>    â”œâ”€ Actor: developer-name
>    â”œâ”€ Issued at: 2024-02-09T10:15:32Z
>    â””â”€ Expires in: 5 minutes (ONE-TIME USE)
> 3. GitHub Actions sends token to AWS STS
> 4. AWS validates:
>    â”œâ”€ Is this really from GitHub? (verify signature)
>    â”œâ”€ Is this repository trusted? (check condition)
>    â””â”€ Does role allow this workflow? (verify principal)
> 5. AWS issues temporary credentials:
>    â”œâ”€ AccessKeyId: temporary
>    â”œâ”€ SecretAccessKey: temporary
>    â”œâ”€ SessionToken: temporary
>    â””â”€ Expires in: 1 hour
>
> **Benefits of OIDC**:
>
> âœ… **No Stored Credentials**
>    - Nothing to leak (token is ephemeral)
>    - Each workflow gets unique token
>    - Can't reuse across workflows
>
> âœ… **Automatic Expiration**
>    - GitHub token: 5 minutes (one-time)
>    - AWS credentials: 1 hour (can't be reused)
>    - Credentials don't persist
>
> âœ… **Fine-Grained Audit Trail**
>    - CloudTrail logs show:
>       â”œâ”€ Which GitHub repo made call
>       â”œâ”€ Which workflow triggered it
>       â”œâ”€ Which branch it ran on
>       â”œâ”€ Exact timestamp
>       â””â”€ Can't do this with stored keys
>
> âœ… **Easy Rotation**
>    - No secrets to rotate
>    - Just trust GitHub if it's still secure
>    - (GitHub maintains OIDC keys, not you)
>
> âœ… **Least Privilege**
>    - Can restrict to specific repos
>    - Can restrict to specific branches
>    - Can restrict to specific workflows
>    - Example: Only allow main branch to deploy to prod
>
> **Security Comparison**:
> ```
> Stored Key:
> â”œâ”€ Lifetime: Unlimited (until rotated)
> â”œâ”€ Scope: Any action that can access secrets
> â”œâ”€ Leak impact: Full AWS access forever
> â””â”€ Audit: Action name, but not definitive
>
> OIDC Token:
> â”œâ”€ Lifetime: 5 minutes (GitHub) â†’ 1 hour (AWS)
> â”œâ”€ Scope: Only this specific workflow execution
> â”œâ”€ Leak impact: Can't reuse (already expired)
> â””â”€ Audit: Complete lineage from GitHub
> ```
>
> **AWS Best Practice**: OIDC is now the recommended approach. It's how enterprise customers do it.
>"

---

### Q4: "How does ArgoCD handle failed deployments?"

**Answer**:
> "ArgoCD has sophisticated failure detection and remediation:
>
> **Scenario**: New image pushed, deployment happens, but app crashes
>
> **Detection Phase**:
> 1. ArgoCD deploys new version via helm upgrade
> 2. Kubernetes creates new pods with new image
> 3. New pod starts but readiness probe fails (app error)
> 4. Pod remains in NotReady state
>
> **Health Assessment**:
> ArgoCD checks application health:
> ```
> Pod 1: Running, but NotReady (readiness probe failed)
>        Status: Progressing/Degraded
> Pod 2: Running, but NotReady (same issue)
>        Status: Progressing/Degraded
> Pod 3: Still Running (old version, still healthy)
>        Status: Healthy
> ```
> *(Note: depends on surge strategy)*
>
> **Automatic Remediation Options**:
>
> **Option 1: Auto-Rollback (if configured)**
> ```
> Application sync policy:
>   syncPolicy:
>     automated:
>       prune: true
>       selfHeal: true
>       allowEmpty: false
> ```
> - ArgoCD detects degradation
> - Rolls back to previous stable version
> - Updates Helm to previous version
> - Pods recover to last known good state
> - Alert sent to team (manual review needed)
>
> **Option 2: Manual Intervention (safer)**
> ```
> Application status: OUT OF SYNC / DEGRADED
> UI shows: Health issues detected
> Team receives alert: \"UI deployment failed\"
> ```
> - Developer checks logs:
>   â”œâ”€ kubectl logs <pod>
>   â”œâ”€ kubectl describe pod <pod>
>   â””â”€ Debugging shows: TypeError in JavaScript (bad build)
> - Fix applied to source code
> - New commit pushed
> - GitHub Actions rebuilds
> - ArgoCD detects new image tag
> - Deployment retries
>
> **Option 3: Progressive Deployment (Canary/Blue-Green)**
> ```yaml
> apiVersion: argoproj.io/v1alpha1
> kind: Application
> metadata:
>   name: ui
>   annotations:
>     argocd.argoproj.io/deployment-strategy: progressive
> spec:
>   syncPolicy:
>     syncStrategy:
>       canary:
>         steps:
>         - weight: 10        # Route 10% to new version
>         - pause: {}         # Wait for metrics
>         - weight: 50        # Route 50% to new version
>         - pause: {}
>         - weight: 100       # Route 100%
> ```
> - Only 10% of traffic gets new version initially
> - Monitor error rates, latency
> - If metrics look good: proceed to 50%
> - If problems detected: auto-revert to 0%
> - Zero customer impact!
>
> **Failure Notification**:
> ArgoCD sends alerts:
> - Slack: 'UI deployment failed, rolling back'
> - Email: detailed failure report
> - Jira: auto-create incident ticket
> - Datadog: integration with monitoring
>
> **Prevention Mechanisms**:
> 1. **Pre-deployment Checks**
>    - GitHub Actions tests before push
>    - Helm chart validation
>    - Security scanning
> 2. **Readiness Probes** (in pod spec)
>    - HTTP GET /health (every 5 seconds)
>    - If fail 2x â†’ not ready
>    - Prevents traffic routing to bad pods
> 3. **Pod Disruption Budgets**
>    - Minimum availability guaranteed
>    - Prevents all pods from being replaced at once
> 4. **Resource Limits**
>    - Prevents OOM crashes
>    - CPU throttling prevents slowdowns
>
> **Philosophy**: Failures are expected in production. Design for graceful degradation, not failure prevention.
>"

---

### Q5: "How do all these systems work together in production?"

**Answer**:
> "It's an integrated ecosystem:
>
> **Development to Production Journey**:
>
> **DAY 1: Setup**
> â”œâ”€ Terraform creates EKS cluster + all add-ons
> â”œâ”€ Karpenter installed for auto-scaling nodes
> â”œâ”€ Helm charts created for each microservice
> â”œâ”€ GitHub Actions workflow defined
> â”œâ”€ ArgoCD deployed and configured
> â””â”€ All pieces in place
>
> **DAY 30: Production Release**
> â”œâ”€ Daily deployments automated
> â”œâ”€ Traffic scales with demand (HPA)
> â”œâ”€ Nodes auto-scale (Karpenter)
> â”œâ”€ Costs optimized (Spot + On-Demand mix)
> â”œâ”€ All observable (OpenTelemetry)
> â””â”€ Zero manual intervention
>
> **REAL TIME FLOW**:
>
> **T+0min**: Traffic surge detected
> â”‚
> â”œâ”€ HPA: CPU usage increases 85% â†’ scale pods 3â†’5
> â”œâ”€ Karpenter: Not enough node capacity
> â””â”€ Karpenter provisions new t3.large (30s)
>
> **T+0:30**: New pods running, traffic stabilized
> â”‚
> â”œâ”€ HPA: CPU now 78% (stable)
> â”œâ”€ Karpenter: Nodes ready, consolidation evaluated
> â””â”€ App responsive, all users happy
>
> **DEPLOYMENT DURING PEAK TRAFFIC**:
>
> **T=peak traffic**: Traffic 100 req/s
> â”‚
> â”œâ”€ Developer pushes UI fix
> â”œâ”€ GitHub Actions: Build new image (1 min)
> â”œâ”€ ArgoCD: Detect new version (30 sec)
> â”œâ”€ Deployment: Rolling update starts
> â”‚  â”œâ”€ Create 1 new pod (surge=1)
> â”‚  â”œâ”€ Wait for readiness (healthy)
> â”‚  â”œâ”€ Old pod terminates gracefully
> â”‚  â””â”€ Repeat for remaining pods
> â”‚
> â”œâ”€ During update: Always 4-5 pods handling traffic
> â”œâ”€ Zero downtime: Requests never drop
> â””â”€ Users don't notice (completely transparent)
>
> **SYSTEM RESILIENCE**:
>
> Failure Scenario: Database connection timeout
> â”‚
> â”œâ”€ Application logs: 'DB connection timeout'
> â”œâ”€ OpenTelemetry: Trace shows latency spike (X-Ray)
> â”œâ”€ CloudWatch: Error rate alert triggered
> â”œâ”€ Metrics: Slow queries detected
> â”œâ”€ Developer: Looks at traces â†’ finds bad query
> â”œâ”€ Quick fix: Deploy new version
> â”‚  â”œâ”€ Code change committed
> â”‚  â”œâ”€ GitHub Actions rebuilds
> â”‚  â”œâ”€ ArgoCD deploys
> â”‚  â””â”€ Fixed in production (2 min)
> â”œâ”€ Monitoring: Error rate returns to normal
> â””â”€ All automated!
>
> **COST OPTIMIZATION**:
>
> â”œâ”€ Off-peak hours (2am-6am): 3 pods (1 on-demand)
> â”‚  â””â”€ Cost: ~\$0.10/hour (minimal)
> â”‚
> â”œâ”€ Business hours (9am-5pm): 20 pods (15 on-demand + 5 Spot)
> â”‚  â””â”€ Cost: ~\$1.50/hour (pay for what you use)
> â”‚
> â”œâ”€ Peak hours: 40 pods (30 on-demand + 10 Spot)
> â”‚  â””â”€ Cost: ~\$3.00/hour (necessary for traffic)
> â”‚
> â””â”€ Monthly: ~\$1,200 (fully managed, auto-scaling)
>
> **COMPLETE AUTOMATION BENEFITS**:
> â”œâ”€ âœ… Time: From code change to production in 2-3 minutes
> â”œâ”€ âœ… Reliability: No human errors in deployment
> â”œâ”€ âœ… Visibility: Complete audit trail in Git
> â”œâ”€ âœ… Safety: Automatic rollback on failures
> â”œâ”€ âœ… Cost: Pay exactly for what you use
> â”œâ”€ âœ… Scalability: From 100 to 10,000 pods transparently
> â””â”€ âœ… Operations: DevOps, not 'DevOps on-call'
>"

---

## Final Summary

You have implemented a **world-class, production-ready Kubernetes infrastructure** demonstrating:

**Infrastructure & Operations**:
- Terraform IaC for reproducible deployments
- EKS cluster with enterprise networking
- All critical add-ons (Pod Identity, Storage, Networking, DNS)

**Application Scaling**:
- Horizontal Pod Autoscaling (HPA) for demand-driven scaling
- Karpenter for intelligent node provisioning
- Cost optimization via Spot instances
- Zero-downtime deployments

**Microservices Architecture**:
- 5 independent, scalable microservices
- AWS data plane integration (RDS, DynamoDB, ElastiCache, SQS)
- Secure secret management
- Persistent storage with EBS

**CI/CD Automation**:
- GitHub Actions for Continuous Integration
- OIDC-based secure AWS access (no hardcoded credentials)
- Docker image building and ECR deployment
- ArgoCD for GitOps-style Continuous Deployment
- Helm for standardized, reproducible deployments

**Observability**:
- OpenTelemetry for distributed tracing
- Complete audit trail of every deployment
- Real-time monitoring and alerting
- Root cause analysis capabilities

This is **enterprise-grade infrastructure** with all the hallmarks of production systems at scale.

